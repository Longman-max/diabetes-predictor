{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84853047",
   "metadata": {
    "id": "84853047"
   },
   "source": [
    "# XGBOOST Version by Precious Kings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1114cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost_diabetes.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a65fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== XGBoost - Risk Stratification (multiclass) ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      4640\n",
      "           1       0.99      0.99      0.99      1381\n",
      "           2       0.96      0.90      0.93        29\n",
      "\n",
      "    accuracy                           1.00      6050\n",
      "   macro avg       0.98      0.96      0.97      6050\n",
      "weighted avg       1.00      1.00      1.00      6050\n",
      "\n",
      "Accuracy: 0.9958677685950413\n",
      "Macro F1: 0.9724161540181487\n",
      "\n",
      "==== XGBoost - Progression (binary) ====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.99      0.90      5016\n",
      "           1       0.42      0.05      0.09      1034\n",
      "\n",
      "    accuracy                           0.83      6050\n",
      "   macro avg       0.63      0.52      0.49      6050\n",
      "weighted avg       0.76      0.83      0.76      6050\n",
      "\n",
      "Accuracy: 0.8261157024793389\n",
      "F1 (binary): 0.08521739130434783\n",
      "\n",
      "Saved metrics_xgboost.csv\n"
     ]
    }
   ],
   "source": [
    "# xgboost_diabetes.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ---------------------------\n",
    "# CONFIG\n",
    "# ---------------------------\n",
    "CSV_PATH = \"../data/new_diabetic_data.csv\"   # <-- change to your path\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Load & basic cleaning\n",
    "# ---------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "# unify missing tokens\n",
    "df = df.replace(\"?\", np.nan)\n",
    "\n",
    "# ensure encounter_id and patient_nbr are usable for sorting\n",
    "df['encounter_id'] = pd.to_numeric(df['encounter_id'], errors='coerce')\n",
    "df['patient_nbr'] = pd.to_numeric(df['patient_nbr'], errors='coerce')\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Map A1Cresult & max_glu_serum to numeric proxies\n",
    "# (these numeric proxies are used to compute risk_score)\n",
    "# ---------------------------\n",
    "a1c_map = {\n",
    "    'None': 0.0, 'Norm': 1.0, '>7': 2.0, '>8': 3.0\n",
    "}\n",
    "glu_map = {\n",
    "    'None': 0.0, 'Norm': 1.0, '>200': 2.0, '>300': 3.0\n",
    "}\n",
    "df['A1C_numeric'] = df['A1Cresult'].astype(str).map(a1c_map)\n",
    "df['max_glu_numeric'] = df['max_glu_serum'].astype(str).map(glu_map)\n",
    "\n",
    "# ---------------------------\n",
    "# 3) Ensure numeric columns are numeric\n",
    "# ---------------------------\n",
    "num_cols = [\n",
    "    'num_lab_procedures', 'num_procedures', 'num_medications',\n",
    "    'number_outpatient', 'number_emergency', 'number_inpatient',\n",
    "    'number_diagnoses', 'time_in_hospital'\n",
    "]\n",
    "for c in num_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Compute risk_score exactly as before and risk_class\n",
    "#    risk_score = 0.4*A1C + 0.3*number_inpatient + 0.2*number_diagnoses + 0.1*num_medications\n",
    "# ---------------------------\n",
    "df['A1C_numeric'] = df['A1C_numeric'].fillna(0.0)\n",
    "for c in ['number_inpatient', 'number_diagnoses', 'num_medications']:\n",
    "    if c not in df.columns:\n",
    "        df[c] = 0\n",
    "    df[c] = df[c].fillna(0)\n",
    "\n",
    "df['risk_score_raw'] = (\n",
    "    0.4 * df['A1C_numeric'] +\n",
    "    0.3 * df['number_inpatient'] +\n",
    "    0.2 * df['number_diagnoses'] +\n",
    "    0.1 * df['num_medications']\n",
    ")\n",
    "\n",
    "# normalize 0-100 (safe: if constant, set to 0)\n",
    "minv = df['risk_score_raw'].min()\n",
    "maxv = df['risk_score_raw'].max()\n",
    "if pd.isna(minv) or pd.isna(maxv) or maxv == minv:\n",
    "    df['risk_score'] = 0.0\n",
    "else:\n",
    "    df['risk_score'] = 100 * (df['risk_score_raw'] - minv) / (maxv - minv)\n",
    "\n",
    "def risk_class(score):\n",
    "    if score <= 40:\n",
    "        return 0  # Fair\n",
    "    elif score <= 70:\n",
    "        return 1  # Moderate\n",
    "    else:\n",
    "        return 2  # High\n",
    "\n",
    "df['risk_class'] = df['risk_score'].apply(risk_class)\n",
    "\n",
    "# ---------------------------\n",
    "# 5) Build progression label using patient_nbr (next visit worse than current)\n",
    "# ---------------------------\n",
    "df = df.sort_values(['patient_nbr', 'encounter_id'])\n",
    "df['next_risk_class'] = df.groupby('patient_nbr')['risk_class'].shift(-1)\n",
    "df['progression'] = (df['next_risk_class'] > df['risk_class']).astype('Int64')  # 1 = worsened, 0 = same/improved\n",
    "\n",
    "# drop last encounters (no next visit)\n",
    "df = df.dropna(subset=['next_risk_class']).copy()\n",
    "df['progression'] = df['progression'].astype(int)\n",
    "\n",
    "# ---------------------------\n",
    "# 6) Feature list: remove identifiers and targets\n",
    "# ---------------------------\n",
    "exclude = {\n",
    "    'encounter_id', 'patient_nbr', 'risk_score_raw', 'risk_score',\n",
    "    'risk_class', 'next_risk_class', 'progression'\n",
    "}\n",
    "features = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "# small safety: remove columns that are entirely NA or non-informative\n",
    "features = [c for c in features if df[c].notna().any()]\n",
    "\n",
    "X = df[features]\n",
    "y_risk = df['risk_class']\n",
    "y_prog = df['progression']\n",
    "\n",
    "# ---------------------------\n",
    "# 7) Preprocessing pipeline (numeric impute+scale, categorical impute+OHE)\n",
    "# ---------------------------\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "], remainder='drop')\n",
    "\n",
    "# ---------------------------\n",
    "# 8) Train-test split (stratify by risk_class so class balance preserved)\n",
    "# ---------------------------\n",
    "X_train, X_test, y_risk_train, y_risk_test, y_prog_train, y_prog_test = train_test_split(\n",
    "    X, y_risk, y_prog, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y_risk\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# 9) XGBoost pipelines for both tasks\n",
    "# ---------------------------\n",
    "# Risk classification (multiclass)\n",
    "xgb_risk = Pipeline(steps=[\n",
    "    ('preproc', preprocessor),\n",
    "    ('model', XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=RANDOM_STATE,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "xgb_risk.fit(X_train, y_risk_train)\n",
    "y_risk_pred = xgb_risk.predict(X_test)\n",
    "\n",
    "# Progression (binary)\n",
    "xgb_prog = Pipeline(steps=[\n",
    "    ('preproc', preprocessor),\n",
    "    ('model', XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=RANDOM_STATE,\n",
    "        tree_method='hist',\n",
    "        verbosity=0\n",
    "    ))\n",
    "])\n",
    "xgb_prog.fit(X_train, y_prog_train)\n",
    "y_prog_pred = xgb_prog.predict(X_test)\n",
    "\n",
    "# ---------------------------\n",
    "# 10) Evaluation & save metrics\n",
    "# ---------------------------\n",
    "print(\"==== XGBoost - Risk Stratification (multiclass) ====\")\n",
    "print(classification_report(y_risk_test, y_risk_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_risk_test, y_risk_pred))\n",
    "print(\"Macro F1:\", f1_score(y_risk_test, y_risk_pred, average='macro'))\n",
    "\n",
    "print(\"\\n==== XGBoost - Progression (binary) ====\")\n",
    "print(classification_report(y_prog_test, y_prog_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_prog_test, y_prog_pred))\n",
    "print(\"F1 (binary):\", f1_score(y_prog_test, y_prog_pred, average='binary'))\n",
    "\n",
    "# Save basic metrics for comparison\n",
    "metrics = {\n",
    "    'model': 'xgboost',\n",
    "    'risk_accuracy': accuracy_score(y_risk_test, y_risk_pred),\n",
    "    'risk_macro_f1': f1_score(y_risk_test, y_risk_pred, average='macro'),\n",
    "    'prog_accuracy': accuracy_score(y_prog_test, y_prog_pred),\n",
    "    'prog_f1': f1_score(y_prog_test, y_prog_pred, average='binary')\n",
    "}\n",
    "pd.DataFrame([metrics]).to_csv(\"metrics_xgboost.csv\", index=False)\n",
    "print(\"\\nSaved metrics_xgboost.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
